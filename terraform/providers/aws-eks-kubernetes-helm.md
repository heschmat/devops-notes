# Managing Kubernetes & Helm with Terraform on EKS

This document explains how Terraform interacts with Amazon EKS using the `kubernetes` and `helm` providers, how this relates to kubeconfig and `kubectl`, and what `aws eks update-kubeconfig` does behind the scenes.

---

## 1. Why the Kubernetes Provider is Needed

When you define Kubernetes resources in Terraform (e.g., `kubernetes_service_account`), Terraform must know how to connect to your cluster. Without a configured provider, Terraform defaults to `http://localhost:80`, which results in:

```
Error: Post "http://localhost/api/v1/...": dial tcp 127.0.0.1:80: connect: connection refused
```

✅ **Solution:** Configure the provider with your EKS connection details:

```hcl
provider "kubernetes" {
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.this.token
}
```

This block is equivalent to what `kubectl` reads from kubeconfig.

---

## 2. Helm Provider

The Helm provider also needs cluster connection info. It uses the same fields as the Kubernetes provider:

```hcl
provider "helm" {
  kubernetes = {
    host                   = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
    token                  = data.aws_eks_cluster_auth.this.token
  }
}
```

Terraform can then install and manage Helm charts directly on your EKS cluster.

---

## 3. Mapping Kubeconfig to Terraform Providers

### Example kubeconfig

```yaml
clusters:
- cluster:
    server: https://ABCDEF123456.gr7.us-east-1.eks.amazonaws.com
    certificate-authority-data: <base64-encoded-CA>
  name: eks-cluster
users:
- name: eks-user
  user:
    token: <JWT token>
```

### Terraform equivalent

```hcl
provider "kubernetes" {
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.this.token
}

provider "helm" {
  kubernetes = {
    host                   = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
    token                  = data.aws_eks_cluster_auth.this.token
  }
}
```

| kubeconfig field                                | Terraform kubernetes provider | Terraform helm provider             |
| ----------------------------------------------- | ----------------------------- | ----------------------------------- |
| `clusters[].cluster.server`                     | `host`                        | `kubernetes.host`                   |
| `clusters[].cluster.certificate-authority-data` | `cluster_ca_certificate`      | `kubernetes.cluster_ca_certificate` |
| `users[].user.token`                            | `token`                       | `kubernetes.token`                  |

---

## 4. Reusing Existing Kubeconfig in Terraform

Instead of wiring `module.eks` outputs, you can point Terraform directly at your kubeconfig:

```hcl
provider "kubernetes" {
  config_path      = "~/.kube/config"
  config_context   = "eks-context" # optional
  load_config_file = true
}

provider "helm" {
  kubernetes {
    config_path      = "~/.kube/config"
    config_context   = "eks-context" # optional
    load_config_file = true
  }
}
```

---

## 5. What `aws eks update-kubeconfig` Does

When you run:

```bash
aws eks update-kubeconfig --name <cluster-name> --region <region>
```

`kubectl` needs the API server address and CA to open a TLS connection — `update-kubeconfig` fetches and writes those values so kubectl can talk to the correct server. 

`kubectl` also needs an authentication mechanism. EKS uses IAM-backed tokens (generated by `aws eks get-token`) — `update-kubeconfig` installs an exec entry so tokens are fetched on demand. Without that, `kubectl` has no way to present a valid EKS IAM token.
---

## ✅ Key Takeaways

* `kubectl` and `helm` use kubeconfig; Terraform needs provider blocks with equivalent info.
* `kubernetes` provider manages raw Kubernetes resources.
* `helm` provider manages Helm charts.
* Both providers need **endpoint + CA cert + auth token**.
* `aws eks update-kubeconfig` simply automates fetching cluster details and writing them into kubeconfig.
